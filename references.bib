
@article{ferrante2012,
	title = {A transparent and transportable methodology for evaluating {Data} {Linkage} software},
	volume = {45},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046411001729},
	doi = {10.1016/j.jbi.2011.10.006},
	abstract = {There has been substantial growth in Data Linkage (DL) activities in recent years. This reflects growth in both the demand for, and the supply of, linked or linkable data. Increased utilisation of DL “services” has brought with it increased need for impartial information about the suitability and performance capabilities of DL software programs and packages. Although evaluations of DL software exist; most have been restricted to the comparison of two or three packages. Evaluations of a large number of packages are rare because of the time and resource burden placed on the evaluators and the need for a suitable “gold standard” evaluation dataset. In this paper we present an evaluation methodology that overcomes a number of these difficulties. Our approach involves the generation and use of representative synthetic data; the execution of a series of linkages using a pre-defined linkage strategy; and the use of standard linkage quality metrics to assess performance. The methodology is both transparent and transportable, producing genuinely comparable results. The methodology was used by the Centre for Data Linkage (CDL) at Curtin University in an evaluation of ten DL software packages. It is also being used to evaluate larger linkage systems (not just packages). The methodology provides a unique opportunity to benchmark the quality of linkages in different operational environments.},
	language = {en},
	number = {1},
	urldate = {2022-07-28},
	journal = {Journal of Biomedical Informatics},
	author = {Ferrante, Anna and Boyd, James},
	month = feb,
	year = {2012},
	keywords = {Data matching, Linkage quality, Medical record linkage, Software evaluation},
	pages = {165--172},
}

@inproceedings{nanayakkara2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Robust {Temporal} {Graph} {Clustering} for {Group} {Record} {Linkage}},
	isbn = {9783030161453},
	doi = {10.1007/978-3-030-16145-3_41},
	abstract = {Research in the social sciences is increasingly based on large and complex data collections, where individual data sets from different domains need to be linked to allow advanced analytics. A popular type of data used in such a context are historical registries containing birth, death, and marriage certificates. Individually, such data sets however limit the types of studies that can be conducted. Specifically, it is impossible to track individuals, families, or households over time. Once such data sets are linked and family trees are available it is possible to, for example, investigate how education, health, mobility, and employment influence the lives of people over two or even more generations. The linkage of historical records is challenging because of data quality issues and because often there are no ground truth data available. Unsupervised techniques need to be employed, which generally are based on similarity graphs generated by comparing individual records. In this paper we present a novel temporal clustering approach aimed at linking records of the same group (such as all births by the same mother) where temporal constraints (such as intervals between births) need to be enforced. We combine a connected component approach with an iterative merging step which considers temporal constraints to obtain accurate clustering results. Experiments on a real Scottish data set show the superiority of our approach over a previous clustering approach for record linkage.},
	language = {en},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Springer International Publishing},
	author = {Nanayakkara, Charini and Christen, Peter and Ranbaduge, Thilina},
	editor = {Yang, Qiang and Zhou, Zhi-Hua and Gong, Zhiguo and Zhang, Min-Ling and Huang, Sheng-Jun},
	year = {2019},
	keywords = {Birth bundling, Entity resolution, Star clustering, Vital records},
	pages = {526--538},
}

@book{zhuang2019,
	title = {Entity {Resolution} with an {Application} to the {El} {Salvadoran} {Conflict}},
	url = {https://dukestatsci.github.io/thesis-sp19-zhuang-entity_resolution/},
	abstract = {Entity Resolution with an Application to the El Salvadoran Conflict},
	urldate = {2022-07-12},
	author = {Zhuang, Bihan},
}

@article{marchant2021,
	title = {Statistical {Approaches} for {Entity} {Resolution} under {Uncertainty}},
	language = {en},
	author = {Marchant, Neil G},
	pages = {185},
}

@article{menestrina2010,
	title = {Evaluating entity resolution results},
	volume = {3},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/1920841.1920871},
	doi = {10.14778/1920841.1920871},
	abstract = {Entity Resolution (ER) is the process of identifying groups of records that refer to the same real-world entity. Various measures (e.g., pairwise F1, cluster F1) have been used for evaluating ER results. However, ER measures tend to be chosen in an ad-hoc fashion without careful thought as to what defines a good result for the specific application at hand. In this paper, our contributions are twofold. First, we conduct an analysis on existing ER measures, showing that they can often conflict with each other by ranking the results of ER algorithms differently. Second, we explore a new distance measure for ER (called "generalized merge distance" or GMD) inspired by the edit distance of strings, using cluster splits and merges as its basic operations. A significant advantage of GMD is that the cost functions for splits and merges can be configured, enabling us to clearly understand the characteristics of a defined GMD measure. Surprisingly, a state-of-the-art clustering measure called Variation of Information is a special case of our configurable GMD measure, and the widely used pairwise F1 measure can be directly computed using GMD. We present an efficient linear-time algorithm that correctly computes the GMD measure for a large class of cost functions that satisfy reasonable properties.},
	number = {1-2},
	urldate = {2022-07-12},
	journal = {Proceedings of the VLDB Endowment},
	author = {Menestrina, David and Whang, Steven Euijong and Garcia-Molina, Hector},
	month = sep,
	year = {2010},
	pages = {208--219},
}

@article{draisbach2019,
	title = {Transforming {Pairwise} {Duplicates} to {Entity} {Clusters} for {High}-quality {Duplicate} {Detection}},
	volume = {12},
	issn = {1936-1955},
	url = {https://doi.org/10.1145/3352591},
	doi = {10.1145/3352591},
	abstract = {Duplicate detection algorithms produce clusters of database records, each cluster representing a single real-world entity. As most of these algorithms use pairwise comparisons, the resulting (transitive) clusters can be inconsistent: Not all records within a cluster are sufficiently similar to be classified as duplicate. Thus, one of many subsequent clustering algorithms can further improve the result. We explain in detail, compare, and evaluate many of these algorithms and introduce three new clustering algorithms in the specific context of duplicate detection. Two of our three new algorithms use the structure of the input graph to create consistent clusters. Our third algorithm, and many other clustering algorithms, focus on the edge weights, instead. For evaluation, in contrast to related work, we experiment on true real-world datasets, and in addition examine in great detail various pair-selection strategies used in practice. While no overall winner emerges, we are able to identify best approaches for different situations. In scenarios with larger clusters, our proposed algorithm, Extended Maximum Clique Clustering (EMCC), and Markov Clustering show the best results. EMCC especially outperforms Markov Clustering regarding the precision of the results and additionally has the advantage that it can also be used in scenarios where edge weights are not available.},
	number = {1},
	urldate = {2022-07-12},
	journal = {Journal of Data and Information Quality},
	author = {Draisbach, Uwe and Christen, Peter and Naumann, Felix},
	month = dec,
	year = {2019},
	keywords = {Record linkage, clustering, data matching, deduplication, entity resolution},
	pages = {3:1--3:30},
}

@article{croset2016,
	title = {Flexible data integration and curation using a graph-based approach},
	volume = {32},
	issn = {1367-4803},
	url = {https://doi.org/10.1093/bioinformatics/btv644},
	doi = {10.1093/bioinformatics/btv644},
	abstract = {Motivation: The increasing diversity of data available to the biomedical scientist holds promise for better understanding of diseases and discovery of new treatments for patients. In order to provide a complete picture of a biomedical question, data from many different origins needs to be combined into a unified representation. During this data integration process, inevitable errors and ambiguities present in the initial sources compromise the quality of the resulting data warehouse, and greatly diminish the scientific value of the content. Expensive and time-consuming manual curation is then required to improve the quality of the information. However, it becomes increasingly difficult to dedicate and optimize the resources for data integration projects as available repositories are growing both in size and in number everyday.Results: We present a new generic methodology to identify problematic records, causing what we describe as ‘data hairball’ structures. The approach is graph-based and relies on two metrics traditionally used in social sciences: the graph density and the betweenness centrality. We evaluate and discuss these measures and show their relevance for flexible, optimized and automated data curation and linkage. The methodology focuses on information coherence and correctness to improve the scientific meaningfulness of data integration endeavors, such as knowledge bases and large data warehouses.Contact:samuel.croset@roche.comSupplementary information:Supplementary data are available at Bioinformatics online.},
	number = {6},
	urldate = {2022-07-12},
	journal = {Bioinformatics},
	author = {Croset, Samuel and Rupp, Joachim and Romacker, Martin},
	month = mar,
	year = {2016},
	pages = {918--925},
}

@article{randall2014,
	title = {Use of graph theory measures to identify errors in record linkage},
	volume = {115},
	issn = {0169-2607},
	url = {https://www.sciencedirect.com/science/article/pii/S0169260714001047},
	doi = {10.1016/j.cmpb.2014.03.008},
	abstract = {Ensuring high linkage quality is important in many record linkage applications. Current methods for ensuring quality are manual and resource intensive. This paper seeks to determine the effectiveness of graph theory techniques in identifying record linkage errors. A range of graph theory techniques was applied to two linked datasets, with known truth sets. The ability of graph theory techniques to identify groups containing errors was compared to a widely used threshold setting technique. This methodology shows promise; however, further investigations into graph theory techniques are required. The development of more efficient and effective methods of improving linkage quality will result in higher quality datasets that can be delivered to researchers in shorter timeframes.},
	language = {en},
	number = {2},
	urldate = {2022-07-12},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Randall, Sean M. and Boyd, James H. and Ferrante, Anna M. and Bauer, Jacqueline K. and Semmens, James B.},
	month = jul,
	year = {2014},
	keywords = {Data quality, Graph theory, Record linkage},
	pages = {55--63},
}

@article{fellegi1969,
	title = {A {Theory} for {Record} {Linkage}},
	volume = {64},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2286061},
	doi = {10.2307/2286061},
	abstract = {A mathematical model is developed to provide a theoretical framework for a computer-oriented solution to the problem of recognizing those records in two files which represent identical persons, objects or events (said to be matched). A comparison is to be made between the recorded characteristics and values in two records (one from each file) and a decision made as to whether or not the members of the comparison-pair represent the same person or event, or whether there is insufficient evidence to justify either of these decisions at stipulated levels of error. These three decisions are referred to as link (A$_{\textrm{1}}$), a non-link (A$_{\textrm{3}}$), and a possible link (A$_{\textrm{2}}$). The first two decisions are called positive dispositions. The two types of error are defined as the error of the decision A$_{\textrm{1}}$ when the members of the comparison pair are in fact unmatched, and the error of the decision A$_{\textrm{3}}$ when the members of the comparison pair are, in fact matched. The probabilities of these errors are defined as μ = ∑$_{\textrm{γεΓ}}$ u(γ)P(A$_{\textrm{1}}$∣γ) and λ = ∑$_{\textrm{γεΓ}}$ m(γ)P(A$_{\textrm{3}}$∣γ) respectively where u(γ), m(γ) are the probabilities of realizing γ (a comparison vector whose components are the coded agreements and disagreements on each characteristic) for unmatched and matched record pairs respectively. The summation is over the whole comparison space Γ of possible realizations. A linkage rule assigns probabilities P(A$_{\textrm{1}}$∣γ), and P(A$_{\textrm{2}}$∣γ), and P(A$_{\textrm{3}}$∣γ) to each possible realization of γ ε Γ. An optimal linkage rule L(μ, λ, Γ) is defined for each value of (μ, λ) as the rule that minimizes P(A$_{\textrm{2}}$) at those error levels. In other words, for fixed levels of error, the rule minimizes the probability of failing to make positive dispositions. A theorem describing the construction and properties of the optimal linkage rule and two corollaries to the theorem which make it a practical working tool are given.},
	number = {328},
	urldate = {2022-07-12},
	journal = {Journal of the American Statistical Association},
	author = {Fellegi, Ivan P. and Sunter, Alan B.},
	year = {1969},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {1183--1210},
}

@article{kirielle2022,
	title = {Unsupervised {Graph}-{Based} {Entity} {Resolution} for {Complex} {Entities}},
	issn = {1556-4681},
	url = {https://doi.org/10.1145/3533016},
	doi = {10.1145/3533016},
	abstract = {Entity resolution (ER) is the process of linking records that refer to the same entity. Traditionally, this process compares attribute values of records to calculate similarities and then classifies pairs of records as referring to the same entity or not based on these similarities. Recently developed graph-based ER approaches combine relationships between records with attribute similarities to improve linkage quality. Most of these approaches only consider databases containing basic entities that have static attribute values and static relationships, such as publications in bibliographic databases. In contrast, temporal record linkage addresses the problem where attribute values of entities can change over time. However, neither existing graph-based ER nor temporal record linkage can achieve high linkage quality on databases with complex entities, where an entity (such as a person) can change its attribute values over time while having different relationships with other entities at different points in time. In this paper we propose an unsupervised graph-based ER framework that is aimed at linking records of complex entities. Our framework provides five key contributions. First, we propagate positive evidence encountered when linking records to use in subsequent links by propagating attribute values that have changed. Second, we employ negative evidence by applying temporal and link constraints to restrict which candidate record pairs to consider for linking. Third, we leverage the ambiguity of attribute values to disambiguate similar records that however belong to different entities. Fourth, we adaptively exploit the structure of relationships to link records that have different relationships. Fifth, using graph measures we refine matched clusters of records by removing likely wrong links between records. We conduct extensive experiments on seven real-world data sets from different domains showing that on average our unsupervised graph-based ER framework can improve precision by up-to 25\% and recall by up-to 29\% compared to several state-of-the-art ER techniques.},
	urldate = {2022-07-12},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Kirielle, Nishadi and Christen, Peter and Ranbaduge, Thilina},
	month = apr,
	year = {2022},
	note = {Just Accepted},
	keywords = {Record linkage, ambiguity., data cleaning, data linkage, dependency graph, temporal data},
}

@article{winkler2014,
	title = {Matching and record linkage},
	volume = {6},
	issn = {1939-0068},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1317},
	doi = {10.1002/wics.1317},
	abstract = {This overview gives background on a number of statistical methods that have been proven effective for record linkage. To prepare data for the main computational algorithms, we need parsing/standardization that allows us to structure the free-form names, addresses, and other fields into corresponding components. The main parameter-estimation methods are unsupervised methods that yield ‘optimal’ record linkage parameters. Extended methods provide estimates of false match rates in both unsupervised and, with greater accuracy, in semi-supervised situations. Finally, the paper describes ongoing research for adjusting standard statistical analyses for linkage error. WIREs Comput Stat 2014, 6:313–325. doi: 10.1002/wics.1317 This article is categorized under: Statistical and Graphical Methods of Data Analysis {\textgreater} EM Algorithm Algorithms and Computational Methods {\textgreater} Seminumerical and Nonnumerical Methods Data: Types and Structure {\textgreater} Data Preparation and Processing Statistical and Graphical Methods of Data Analysis {\textgreater} Markov Chain Monte Carlo (MCMC)},
	language = {en},
	number = {5},
	urldate = {2022-07-12},
	journal = {WIREs Computational Statistics},
	author = {Winkler, William E.},
	year = {2014},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1317},
	keywords = {classification rules, entity resolution, false match and nonmatch rates, string comparison, unsupervised learning},
	pages = {313--325},
}

@misc{yancey2004,
	title = {Improving {EM} {Algorithm} {Estimates} for {Record} {Linkage} {Parameters}},
	url = {https://www.census.gov/library/working-papers/2004/adrm/rrs2004-01.html},
	abstract = {Improving EM Algorithm Estimates for Record Linkage Parameters},
	urldate = {2022-07-28},
	journal = {{US Census Bureau}},
	author = {Yancey, WE},
	year = {2004},
}

@incollection{farrow2015,
	title = {Using graph databases to manage linked data},
	isbn = {9781119072454},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119072454.ch7},
	abstract = {By storing record data and the relationships between the records explicitly in a graph database a technology specifically designed to store 'natural' graph structures and query such information many shortcomings of the relational approach can be avoided, and new approaches to managing and exploiting linked data are enabled. This chapter compares a relational and a graph-based approach, and provides an overview of how linked record data can be managed using a graph database based on the experience gleaned in building such a system at SANT DataLink in South Australia: the Next Generation Linkage Management System (NGLMS). Whereas algorithms based on relational structures and comparisons have a complexity greater than linear complexity, the graph partitioning algorithms have a complexity that is linear on the number of nodes involved. The graph-based approach most naturally represents and allows the exploitation of which it is most desirable to capture the relationship between data items.},
	language = {en},
	urldate = {2022-07-28},
	booktitle = {Methodological {Developments} in {Data} {Linkage}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Farrow, James M.},
	year = {2015},
	doi = {10.1002/9781119072454.ch7},
	keywords = {NGLMS, SANT DataLink, algorithm implementation, graph databases, graph storage approach, linked data, relational approach},
	pages = {125--169},
}

@misc{splink_syn2021,
	title = {Splink synthetic data},
	url = {https://github.com/moj-analytical-services/splink_synthetic_data/releases/tag/v1},
	urldate = {2022-07-29},
	author = {{MoJ Analytical Services team}},
	year = {2021},
}

@misc{wikidata,
	title = {Wikidata},
	url = {https://www.wikidata.org},
	urldate = {2022-07-29},
	author = {{Wikimedia Foundation}},
	year = {2022},
}


@article{burton2018,
	title = {Do healthcare services behave as complex systems? {Analysis} of patterns of attendance and implications for service delivery},
	volume = {16},
	issn = {1741-7015},
	url = {https://doi.org/10.1186/s12916-018-1132-5},
	doi = {10.1186/s12916-018-1132-5},
	number = {1},
	journal = {BMC Medicine},
	author = {Burton, Christopher and Elliott, Alison and Cochran, Amanda and Love, Tom},
	month = sep,
	year = {2018},
	pages = {138},
}

@article{burton2022,
	title = {Frequent attendance at the emergency department shows typical features of complex systems: analysis of multicentre linked data},
	volume = {39},
	copyright = {© Author(s) (or their employer(s)) 2022. No commercial re-use. See rights and permissions. Published by BMJ.},
	issn = {1472-0205, 1472-0213},
	shorttitle = {Frequent attendance at the emergency department shows typical features of complex systems},
	url = {https://emj.bmj.com/content/39/1/3},
	doi = {10.1136/emermed-2020-210772},
	language = {en},
	number = {1},
	urldate = {2022-08-16},
	journal = {Emergency Medicine Journal},
	author = {Burton, Christopher and Stone, Tony and Oliver, Phillip and Dickson, Jon M. and Lewis, Jen and Mason, Suzanne M.},
	month = jan,
	year = {2022},
	pmid = {34039641},
	note = {Publisher: BMJ Publishing Group Ltd and the British Association for Accident \& Emergency Medicine
Section: Original research},
	keywords = {access to care, emergency care systems},
	pages = {3--9},
	file = {Full Text PDF:C\:\\Users\\cm1avs\\Zotero\\storage\\7XZ8RM27\\Burton et al. - 2022 - Frequent attendance at the emergency department sh.pdf:application/pdf;Snapshot:C\:\\Users\\cm1avs\\Zotero\\storage\\AVHPTR6I\\3.html:text/html},
}

@article{sivarajah2017,
	title = {Critical analysis of {Big} {Data} challenges and analytical methods},
	volume = {70},
	issn = {0148-2963},
	url = {https://www.sciencedirect.com/science/article/pii/S014829631630488X},
	doi = {10.1016/j.jbusres.2016.08.001},
	urldate = {2022-08-17},
	journal = {Journal of Business Research},
	author = {Sivarajah, Uthayasankar and Kamal, Muhammad Mustafa and Irani, Zahir and Weerakkody, Vishanth},
	month = jan,
	year = {2017},
	keywords = {Big Data, Big Data Analytics, Challenges, Methods, Systematic literature review},
	pages = {263--286},
}

@inproceedings{christen2007,
  title={A two-step classification approach to unsupervised record linkage},
  author={Christen, Peter},
  booktitle={Proceedings of the sixth Australasian conference on Data mining and analytics-Volume 70},
  pages={111--119},
  year={2007},
  organization={Citeseer}
}

@article{abril2012,
  title={Improving record linkage with supervised learning for disclosure risk assessment},
  author={Abril, Daniel and Navarro-Arribas, Guillermo and Torra, Vicen{\c{c}}},
  journal={Information Fusion},
  volume={13},
  number={4},
  pages={274--284},
  year={2012},
  publisher={Elsevier}
}

@article{steorts2016,
  title={A Bayesian approach to graphical record linkage and deduplication},
  author={Steorts, Rebecca C and Hall, Rob and Fienberg, Stephen E},
  journal={Journal of the American Statistical Association},
  volume={111},
  number={516},
  pages={1660--1672},
  year={2016},
  publisher={Taylor \& Francis}
}

@article{ohare2019,
  title={A review of unsupervised and semi-supervised blocking methods for record linkage},
  author={O’Hare, Kevin and Jurek-Loughrey, Anna and Campos, Cassio de},
  journal={Linking and Mining Heterogeneous and Multi-view Data},
  pages={79--105},
  year={2019},
  publisher={Springer}
}


@article{tromp2011,
	title = {Results from simulated data sets: probabilistic record linkage outperforms deterministic record linkage},
	volume = {64},
	issn = {0895-4356},
	shorttitle = {Results from simulated data sets},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435610002258},
	doi = {10.1016/j.jclinepi.2010.05.008},
	abstract = {Objective
To gain insight into the performance of deterministic record linkage (DRL) vs. probabilistic record linkage (PRL) strategies under different conditions by varying the frequency of registration errors and the amount of discriminating power.
Study Design and Setting
A simulation study in which data characteristics were varied to create a range of realistic linkage scenarios. For each scenario, we compared the number of misclassifications (number of false nonlinks and false links) made by the different linking strategies: deterministic full, deterministic N-1, and probabilistic.
Results
The full deterministic strategy produced the lowest number of false positive links but at the expense of missing considerable numbers of matches dependent on the error rate of the linking variables. The probabilistic strategy outperformed the deterministic strategy (full or N−1) across all scenarios. A deterministic strategy can match the performance of a probabilistic approach providing that the decision about which disagreements should be tolerated is made correctly. This requires a priori knowledge about the quality of all linking variables, whereas this information is inherently generated by a probabilistic strategy.
Conclusion
PRL is more flexible and provides data about the quality of the linkage process that in turn can minimize the degree of linking errors, given the data provided.},
	language = {en},
	number = {5},
	urldate = {2022-08-19},
	journal = {Journal of Clinical Epidemiology},
	author = {Tromp, Miranda and Ravelli, Anita C. and Bonsel, Gouke J. and Hasman, Arie and Reitsma, Johannes B.},
	month = may,
	year = {2011},
	keywords = {Data quality, Deterministic linkage, Medical record linkage, Probabilistic linkage, Registries, Simulation study},
	pages = {565--572},
}

@article {hong2013,
	author = {Hong, Matthew K H and Yao, Henry H I and Pedersen, John S and Peters, Justin S and Costello, Anthony J and Murphy, Declan G and Hovens, Christopher M and Corcoran, Niall M},
	title = {Error rates in a clinical data repository: lessons from the transition to electronic data transfer{\textemdash}a descriptive study},
	volume = {3},
	number = {5},
	elocation-id = {e002406},
	year = {2013},
	doi = {10.1136/bmjopen-2012-002406},
	publisher = {British Medical Journal Publishing Group},
	abstract = {Objective Data errors are a well-documented part of clinical datasets as is their potential to confound downstream analysis. In this study, we explore the reliability of manually transcribed data across different pathology fields in a prostate cancer database and also measure error rates attributable to the source data. Design Descriptive study. Setting Specialist urology service at a single centre in metropolitan Victoria in Australia. Participants Between 2004 and 2011, 1471 patients underwent radical prostatectomy at our institution. In a large proportion of these cases, clinicopathological variables were recorded by manual data-entry. In 2011, we obtained electronic versions of the same printed pathology reports for our cohort. The data were electronically imported in parallel to any existing manual entry record enabling direct comparison between them. Outcome measures Error rates of manually entered data compared with electronically imported data across clinicopathological fields. Results 421 patients had at least 10 comparable pathology fields between the electronic import and manual records and were selected for study. 320 patients had concordant data between manually entered and electronically populated fields in a median of 12 pathology fields (range 10{\textendash}13), indicating an outright accuracy in manually entered pathology data in 76\% of patients. Across all fields, the error rate was 2.8\%, while individual field error ranges from 0.5\% to 6.4\%. Fields in text formats were significantly more error-prone than those with direct measurements or involving numerical figures (p\&lt;0.001). 971 cases were available for review of error within the source data, with figures of 0.1{\textendash}0.9\%. Conclusions While the overall rate of error was low in manually entered data, individual pathology fields were variably prone to error. High-quality pathology data can be obtained for both prospective and retrospective parts of our data repository and the electronic checking of source pathology data for error is feasible.},
	issn = {2044-6055},
	URL = {https://bmjopen.bmj.com/content/3/5/e002406},
	eprint = {https://bmjopen.bmj.com/content/3/5/e002406.full.pdf},
	journal = {BMJ Open}
}






@article{lambert,
  AUTHOR =       {P. C. Lambert and A. J. Sutton and P. R. Burton and K. R. Abrams and D. R. Jones},
  TITLE =        {How vague is vague?  {A} simulation study of the impact of the use of vague prior distributions in {MCMC} using {WinBUGS}},
  JOURNAL =      {Statistics in Medicine},
  YEAR =         {2005},
  volume =       {24},
  pages =        {2401--2428}
}

@incollection{dellas,
  AUTHOR =       {P. Dellaportas and J. J. Foster and I. Ntzoufras},
  TITLE =        {Bayesian variable selection using the {G}ibbs sampling},
  BOOKTITLE =    {Generalized linear models: a {B}ayesian perspective},
  PUBLISHER =    {Marcel Dekker},
  YEAR =         {2000},
  editor =       {D. K. Dey, S. K. Ghosh and B. K. Mallick},
  address =      {New York},
  pages =        {273--286}
}

@book{Williams,
  Author={Williams, D.},
  Year= {1991},
  Title={Probability with Martingales},
  Publisher={Cambridge University Press},
  ISBN={0-521-40605-6}
}
@book{Fristedt,
  Author={Fristedt, B. E. and Gray, L. F.},
  Year={1996},
  Title={A modern approach to probability theory},
  Publisher={Birkh\"{a}user Boston},
  ISBN={0-8176-3807-5}
}

@Book{xie2015,
  title = {Dynamic Documents with {R} and knitr},
  author = {Yihui Xie},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2015},
  edition = {2nd},
  note = {ISBN 978-1498716963},
  url = {http://yihui.name/knitr/},
}
